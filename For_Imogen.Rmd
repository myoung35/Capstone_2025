---
title: "Imogen this is ones for you"
author: "Madalyn Young"
date: "2025-04-09"
output: 
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes # makes the toc move along
    code_folding: hide  # Use "hide" to collapse code by default
editor_options: 
  chunk_output_type: inline
---


```{r warning  = FALSE, collapse=TRUE }
pacman::p_load(tidyverse, scales, dplyr, corrr, janitor, tidyr, psych, readr, lubridate, rpart, rpart.plot, caret, C50, sf, maps, dbscan, geosphere, nnet, randomForest,readxl, tsibble, ggplot2, forecast, tseries, lme4, performance, yardstick, purr,lmerTest)
```


# Load Orignal Data
```{r warning = FALSE, collapse=TRUE}
CustomerProfileData <- read.csv("Data/customer_profile.csv")  
TransactionalData <- read.csv("Data/transactional_data.csv")
AddressZipData <- read.csv("Data/customer_address_and_zip_mapping.csv")
DeliveryCostData <- read.csv("Data/delivery_cost_data.csv")
```

# Data Cleaning - From Final


```{r address clean, warning = FALSE, collapse=TRUE}
#clean the address data
# Split the column
AddressZipData <- AddressZipData |>
  separate(full.address, into = c("ZIP", "City", "State Name", "State Short", 
                                  "County","Code", "Latitude", "Longitude"), sep = ",")

AddressZipData$Latitude <- as.numeric(AddressZipData$Latitude)

AddressZipData$Longitude <- as.numeric(AddressZipData$Longitude)
```


```{r transaction cleaning, warning = FALSE, collapse=TRUE}
TransactionalData$TRANSACTION_DATE <- mdy(TransactionalData$TRANSACTION_DATE)
  

TransactionalData <- TransactionalData %>% 
  mutate(Quarter_column = quarter(TRANSACTION_DATE), 
         Quarter_year = paste(Quarter_column, YEAR, sep = " "),
         MONTH = month(TRANSACTION_DATE)) %>% 
  select(-c(LOADED_CASES, DELIVERED_CASES, LOADED_GALLONS, DELIVERED_GALLONS))
```


```{r cust clean, collapse=FALSE}
#clean Customer Profile Data
  CustomerProfileData <-  CustomerProfileData %>% 
  mutate(
    Entity_ID = case_when(
      is.na(PRIMARY_GROUP_NUMBER) ~ CUSTOMER_NUMBER,  # If PRIMARY_GROUP_NUMBER is NA, use CUSTOMER_NUMBER
      TRUE ~ PRIMARY_GROUP_NUMBER),
    ON_BOARDING_DATE = mdy(ON_BOARDING_DATE),
    FIRST_DELIVERY_DATE = mdy(FIRST_DELIVERY_DATE),
    ON_BOARDING_YEAR = year(ON_BOARDING_DATE),
    FIRST_DELIVERY_YEAR = year(FIRST_DELIVERY_DATE))

char_col <- sapply(CustomerProfileData, is.character)
CustomerProfileData[char_col] <- lapply(CustomerProfileData[char_col], as.factor)

logical_cols <- sapply(CustomerProfileData, is.logical)
CustomerProfileData[logical_cols] <- lapply(CustomerProfileData[logical_cols], as.numeric)

#remove the customer where their on_boarding date was first delivery date was before the onboarding date (1 customer)
CustomerProfileData <- CustomerProfileData %>% 
  filter(FIRST_DELIVERY_DATE>=ON_BOARDING_DATE)
```



```{r aggregatecost, collapse=TRUE}
#Pivot wide the cost data
#aggregated  transaction data to join to customer table

# aggregate transaction data by customer_number and year
#sum the ordered cases and gallons by customer number and year
#this table is set up so each customer number has  two rows, one for 2023, one for 2024. Each column is sum of ordered cases/loaded cases. delivered cases in that year 
aggregated_cost <- TransactionalData |>
  group_by(CUSTOMER_NUMBER, YEAR) |>
  summarize(orderedCases = sum(ORDERED_CASES),
            orderedGallons = sum(ORDERED_GALLONS))



#The code pivots the database above to have one row per customer and a column for each cases/gallons ordered for each year
aggregated_cost_wide <- aggregated_cost |>
  pivot_wider(
    names_from = YEAR, 
    values_from = c(orderedCases, 
                    orderedGallons),
    names_sep = "_"
  )
```


```{r custloc join, collapse=TRUE}
CustomerProfile_Location <- CustomerProfileData %>% 
  left_join(AddressZipData, by = c("ZIP_CODE"="zip")) 
```

```{r locclust, collapse = TRUE}
#cluster the addresses and calculate the centroid for each cluster
##Multiple centroids
set.seed(123)

kmeans_result <- kmeans(CustomerProfile_Location[,c("Longitude", "Latitude")], centers = 4)

CustomerProfile_Location$cluster <- as.factor(kmeans_result$cluster)


centroids <- CustomerProfile_Location %>% 
  group_by(cluster) %>% 
  summarize(centroid_lon = mean(Longitude), centroid_lat = mean(Latitude))

```


```{r calc distance, collapse=TRUE}

haversine_distance <- function(lon1, lat1, lon2, lat2) {
  distHaversine(c(lon1, lat1), c(lon2,lat2))/1609.34 
}# converts meters to miles

#Join main customer data to the clusters created above
CustomerProfile_Location <- CustomerProfile_Location %>% 
  left_join(centroids, by = "cluster")


CustomerProfile_Location <- CustomerProfile_Location %>% 
  mutate(
    distance_to_centroid = mapply(haversine_distance, CustomerProfile_Location$Longitude, CustomerProfile_Location$Latitude, CustomerProfile_Location$centroid_lon, CustomerProfile_Location$centroid_lat)
  )

```

## Annual Customer No Retailer Dataset
```{r annualCust Filtering}
Annual_Customer_No_Retailer <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_wide, by = "CUSTOMER_NUMBER") %>% 
  mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  filter(!(year(FIRST_DELIVERY_DATE) == 2023 & orderedCases_2023 == 0 & orderedGallons_2023 == 0) &
    !(year(FIRST_DELIVERY_DATE) == 2024 & orderedCases_2024 == 0 & orderedGallons_2024 == 0)) %>% 
  filter(!(total_ordered_2023 ==0 & total_ordered_2024 == 0))

```


```{r Annual No Retailer Dataset, collapse=FALSE}
Annual_Customer_No_Retailer <- Annual_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  summarize(FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
            COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
            TRADE_CHANNEL = first(TRADE_CHANNEL),
            SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
            FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
            FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
            ON_BOARDING_DATE = min(ON_BOARDING_DATE),
            ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
            customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            numberOfOutlets = sum(case_when(is.na(PRIMARY_GROUP_NUMBER)~ 1,TRUE ~1)),
            LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
            CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
            
            hasOrderedCases = as.integer(mean(case_when((orderedCases_2023 + orderedCases_2024)>0 ~1, TRUE ~ 0))>0,1,TRUE~0),
            
            propCases = sum(orderedCases_2023, orderedCases_2024)/ sum(total_ordered),
 
            zip_code =  first(ZIP), 
            
            city =  first(City),

            state =   
              first(`State Name`), 

            region = first(cluster),
             
            distance_from_centroid = first(distance_to_centroid),

            total_ordered_2023 = case_when(sum(total_ordered_2023)==0~1,TRUE~sum(total_ordered_2023)),
            total_ordered_2024 = sum(total_ordered_2024),
            percentChangeYOY = ((total_ordered_2024) - (total_ordered_2023))/(total_ordered_2023)) %>% 
  mutate(Binning_column = case_when(
    (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY < 0.10 ~ "low volume low growth",
         (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY >= 0.10 ~ "low volume high growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY < 0.05 ~ "high volume low growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY >=0.05 ~ "high volume high growth",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY > 0 ~ "transtionary growing",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY <= 0 ~ "transitionary declining" )) %>% 
  filter(numberOfOutlets == 1) 

```




```{r unaggragate transactions, collapse=TRUE}
aggregated_cost_by_month <- TransactionalData |>
  group_by(CUSTOMER_NUMBER, YEAR, Quarter_column, Quarter_year,MONTH) |>
  summarize(orderedCases = sum(ORDERED_CASES),
            orderedGallons = sum(ORDERED_GALLONS),
            totalOrdered = sum(ORDERED_CASES, ORDERED_GALLONS))
```


```{r unagjoinCust, collapes = TRUE}
UnaggregatedDates_Customer_No_Retailer <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_by_month, by = "CUSTOMER_NUMBER") %>% 
   mutate(across(c(orderedCases, orderedGallons, totalOrdered), ~ replace_na(.x, 0))) 
  
```

```{r months, collapse=TRUE}
#create rows for each month
all_months <- crossing(
  CUSTOMER_NUMBER = unique(UnaggregatedDates_Customer_No_Retailer$CUSTOMER_NUMBER),
  YEAR = unique(UnaggregatedDates_Customer_No_Retailer$YEAR, na.rm = TRUE),
  MONTH = 1:12
) %>%
  mutate(
    Quarter_column = case_when(
      MONTH >= 1 & MONTH <= 3 ~ 1,
      MONTH >= 4 & MONTH <= 6 ~ 2,
      MONTH >= 7 & MONTH <= 9 ~ 3,
      TRUE ~ 4
    ),
    Quarter_year = paste(Quarter_column, YEAR, sep = " ")
  ) %>% filter(!is.na(YEAR) )

# First, create a customer reference dataset with one row per customer
customer_reference <- UnaggregatedDates_Customer_No_Retailer %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarize(
    PRIMARY_GROUP_NUMBER = first(PRIMARY_GROUP_NUMBER),
    FREQUENT_ORDER_TYPE = first(FREQUENT_ORDER_TYPE),
    FIRST_DELIVERY_DATE = first(FIRST_DELIVERY_DATE),
    ON_BOARDING_DATE = first(ON_BOARDING_DATE),
    COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
    TRADE_CHANNEL = first(TRADE_CHANNEL),
    SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
    LOCAL_MARKET_PARTNER = first(LOCAL_MARKET_PARTNER),
    CO2_CUSTOMER = first(CO2_CUSTOMER),
    ZIP_CODE = first(ZIP_CODE),
    Entity_ID = first(Entity_ID),
    ON_BOARDING_YEAR = first(ON_BOARDING_YEAR),
    FIRST_DELIVERY_YEAR = first(FIRST_DELIVERY_YEAR),
    ZIP = first(ZIP),
    City = first(City),
    `State Name` = first(`State Name`),
    Latitude = first(Latitude),
    Longitude = first(Longitude),
    cluster = first(cluster),
    distance_to_centroid = first(distance_to_centroid)
  )

# Now join the transactions with all_months first, then join with customer reference

UnaggregatedDates_Customer_No_Retailer <- all_months %>%
  left_join(
    UnaggregatedDates_Customer_No_Retailer %>% 
      select(CUSTOMER_NUMBER, YEAR, MONTH, Quarter_column, Quarter_year, orderedCases, orderedGallons, totalOrdered),
    by = c("CUSTOMER_NUMBER", "YEAR", "MONTH")
  ) %>%
  mutate(across(c(orderedCases, orderedGallons, totalOrdered), ~ replace_na(.x, 0))) %>%
  # Join with the customer reference data
  left_join(customer_reference, by = "CUSTOMER_NUMBER")

```

## By Quarter Dataset
```{r By Quarter Data, collapse=FALSE}
BYQUARTER_Customer_No_Retailer <- UnaggregatedDates_Customer_No_Retailer %>% 
  filter(is.na(PRIMARY_GROUP_NUMBER)) %>% 
  group_by(Entity_ID, Quarter_year.x) %>% 
  reframe(
    FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
    COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
    TRADE_CHANNEL = first(TRADE_CHANNEL),
    SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
    FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
    FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
    ON_BOARDING_DATE = min(ON_BOARDING_DATE),
    ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
    customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            
    LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
    CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
    
    hasOrderedCases = as.integer(case_when((orderedCases)>0 ~1, TRUE ~ 0)),
            
    propCases = sum(orderedCases)/ sum(totalOrdered),
    
    zip_code =  first(ZIP), 
            
    city =  first(City),

    state =  first(`State Name`), 

    region = first(cluster),
             
    distance_from_centroid = first(distance_to_centroid),
    orderedCases = sum(orderedCases),
    orderedGallons = sum(orderedGallons),
    totalOrdered = sum(totalOrdered)) %>% 
   mutate(date = as.Date(paste0((as.integer(sub(" .*", "", Quarter_year.x)) - 1) * 3 + 1, 
                               "/1/", sub(".* ", "", Quarter_year.x)), 
                        format="%m/%d/%Y")) 
```

```{r edits to By Quarter, collapse = FALSE}
BYQUARTER_Customer_No_Retailer$quarter <- quarter(BYQUARTER_Customer_No_Retailer$date)
BYQUARTER_Customer_No_Retailer$quarter <- factor(paste0("Q", BYQUARTER_Customer_No_Retailer$quarter))
```

# Model 1
Linear regression using quarter as a factor predictor variable. 

This model was to identify significance and impact of each quarter. Though it did not improve R2 very well, I started to see impacts of Q3
```{r model1, collapse=FALSE}
model1 <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = BYQUARTER_Customer_No_Retailer)


summary(model1)
```

# Model 2
This is my Arima model by quarter
```{r convert Quarter to TS object, collapse=FALSE}
ts_dataQ <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYQUARTER_Customer_No_Retailer$date)), month(min(BYQUARTER_Customer_No_Retailer$date))), 
     frequency = 4) 
```

```{r}
autoplot(ts_dataQ)
```


```{r test for stationary, collapse=FALSE}
adf.test(ts_dataQ)
```

data is stationary and there is no need for differencing!!! this means that the data does not have trends or seasonality that would make it difficult to predict. What I clearly see from the graph. there is one quarter each year where ordering takes place and we can predict that

```{r build the arima, collapse=FALSE}

#these are the tests to build my arima model
acf(ts_dataQ) # 1 significant spike q= 1
pacf(ts_dataQ) # 0 significant spike p= 0

#Arima
QArima <- arima(ts_dataQ, order = c(0,0,1))

summary(QArima)

```
**for the specified arima**
1. Coefficients:
ma1 (-0.9980): This is the first-order moving average coefficient. A value close to -1 suggests that the series has strong negative autocorrelation at lag 1, meaning that an increase in one period tends to be followed by a decrease in the next.

Intercept (4,369,738.19): This represents the mean level of the series (assuming no differencing was applied).

2. Standard Errors (s.e.):
ma1 standard error (0.4149): Measures the uncertainty around the MA(1) estimate.

Intercept standard error (48,231.29): Suggests that the intercept estimate has a relatively small uncertainty compared to its magnitude.

3. Model Fit Metrics:
sigma² (1.398e+11): This is the estimated variance of the residuals.

Log-likelihood (-115.1): Higher values (less negative) suggest a better fit.

AIC (236.19): A lower AIC indicates a better model fit relative to others.

4. Training Set Error Measures:
ME (-86,206.46): Mean error, showing a slight underestimation bias.

RMSE (373,931.1): Measures how much the predicted values deviate from actual values on average.

MAE (346,158.9): Mean absolute error, showing the typical size of forecast errors.

MPE (-2.70%): Mean percentage error, indicating the direction of bias (slight underestimation).

MAPE (8.11%): Mean absolute percentage error, meaning your model's predictions are, on average, 8.11% off.

MASE (0.5429): Mean absolute scaled error; values below 1 suggest better performance than a naive forecast.

ACF1 (0.1830): Autocorrelation of residuals at lag 1. A low value suggests that residuals are fairly uncorrelated (which is good).

Interpretation:
Your MA(1) coefficient is close to -1, suggesting strong negative autocorrelation.

The AIC is reasonable, though you might want to compare it with other models (e.g., ARIMA(1,0,1), ARIMA(0,1,1)).

The MAPE of 8.11% indicates a fairly good predictive performance.

The residual autocorrelation (ACF1 = 0.183) is low, which means the model does not leave much pattern in the residuals.

# Model 3
This is an auto arima to compare to the arima I build manually above
```{r second arima for comp, collapse=FALSE}
#the above model is better
QArimaAuto <- auto.arima(ts_dataQ)
summary(QArimaAuto)
```


# Model 4
This model I tried to create as a way to include both quarter and the binning column to see how it would help a linear regression model

R2 is 0.22, better than a lot of models I had built by this point

```{r join to get bin, collapse=TRUE}
BYQUARTER_Customer_No_Retailer <- BYQUARTER_Customer_No_Retailer %>% 
  left_join(Annual_Customer_No_Retailer %>% select(Entity_ID, Binning_column), by = "Entity_ID")
```


```{r}
model4 <- lm((totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER*CO2_CUSTOMER + propCases + distance_from_centroid + quarter +Binning_column , data = BYQUARTER_Customer_No_Retailer)


summary(model4)
```

# Model 5
This model is an adjustment of the top by logging total orders. I did this to account for the skewedness of total orders and help the predicitability of the model that way. This was successful. This was my best model before moving to MLM
```{r}
model5 <- lm(log(totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER*CO2_CUSTOMER + propCases + distance_from_centroid + quarter +Binning_column , data = BYQUARTER_Customer_No_Retailer)


summary(model5)
```


***I did modeling in the bins, just regressions, a lot of the same as well. All were not well performing. The best performing models were for well performing customers, confirming the issue I had that high volume customers were driving predictability and R2 of my main consolidated models.**

**Let me know if you want me to add any of these for our swire submission**

# Model 6
This is my final MLM model. I did a handful of others, mostly I had more factor variables in my model. not many were significant and when I tried cross validating it would error out due to mismatched levels. I also was not including percent change until this model. I also tried mlm's with customers ordering less than 400 and saw poorer performance

```{r}
Annual_Customer_No_Retailer$Binning_column <- as.factor(Annual_Customer_No_Retailer$Binning_column)

Annual_Customer_No_Retailer <- Annual_Customer_No_Retailer %>% 
  filter(Entity_ID != 4445)


set.seed(123)

# you enter the proportion for the split here. I'd suggest .8
inTrainMLM3<- createDataPartition(Annual_Customer_No_Retailer$total_ordered_2024, p=.8, list=FALSE)

# use the row indexes from line 87 to create the 2 sets.
# train includes the index, test excludes the index.

train_setMLM3 <- Annual_Customer_No_Retailer[inTrainMLM3,]
test_setMLM3 <- Annual_Customer_No_Retailer[-inTrainMLM3,]

```


```{r}
MLMTrain3 <- lmer(total_ordered_2024~customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + percentChangeYOY +  (1|Binning_column), data = train_setMLM3)

```

```{r}
summary(MLMTrain3)
```

```{ predictions}
test_setMLM3$predictions <- pmax(predict(MLMTrain3, newdata =test_setMLM3, allow.new.levels = TRUE),0)
```


```{r rmse}

#this is RMSE 
sqrt(mean((test_setMLM3$total_ordered_2024 - test_setMLM3$predictions)^2))

```

```{r r2}
rsq <- cor(test_setMLM3$total_ordered_2024, test_setMLM3$predictions)^2

print(rsq)

r2(MLMTrain3)

ranef(MLMTrain3)$Binning_column
```
```{r}
test_setMLM3 %>% 
  filter(total_ordered_2024<=10000) %>% 
ggplot(aes(x = total_ordered_2024, y = predictions)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(x = "Actual", y = "Predicted") +
  scale_y_continuous(labels = label_number(scale = 1e-3, suffix = "K"))+
  scale_x_continuous(labels = label_number(scale = 1e-3, suffix = "K"))+
  theme_minimal()+
   theme( 
    panel.grid = element_blank(), 
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)) 
```

```{r}
car::vif(MLMTrain3)
```
vif of 5 means there is no signficant multicollinearity.

I guess what this means is that orders and % change are so variable that the bins themselves dont capture the correlation very heavily 

"This means keeping both variables in the model could still be informative—one capturing broad customer segmentation and the other providing a more continuous measure of change. Sounds like a solid approach!"

***Cross Validation of main MLM Model***

Just as a note, the cross validation is how I identified customer 4445 needed to be filtered out
```{r}
# First, ensure all factors have consistent levels throughout your dataset
factor_vars <- c("FREQUENT_ORDER_TYPE", "COLD_DRINK_CHANNEL", "TRADE_CHANNEL", "Binning_column")

for(var in factor_vars) {
  if(is.factor(Annual_Customer_No_Retailer[[var]]) || is.character(Annual_Customer_No_Retailer[[var]])) {
    Annual_Customer_No_Retailer[[var]] <- factor(Annual_Customer_No_Retailer[[var]])
    print(paste("Factor", var, "has", length(levels(Annual_Customer_No_Retailer[[var]])), "levels"))
  }
}

# Store original factor levels
original_levels <- list()
for(var in factor_vars) {
  if(is.factor(Annual_Customer_No_Retailer[[var]])) {
    original_levels[[var]] <- levels(Annual_Customer_No_Retailer[[var]])
  }
}
```



```{r}
set.seed(1234)
folds <- createFolds(unique(Annual_Customer_No_Retailer$total_ordered_2024), k = 5, list = TRUE, returnTrain = FALSE)

# Initialize storage for performance metrics
cv_rmse <- numeric(length(folds))
cv_mae <- numeric(length(folds))
cv_r2 <- numeric(length(folds))

```


```{r}
for(i in seq_along(folds)) {
  # Create test set indices
  
  test_indices <- folds[[i]]
  
  # Train and test sets
  train_data <- Annual_Customer_No_Retailer[-test_indices, ]
  test_data <- Annual_Customer_No_Retailer[test_indices, ]
  
  # IMPORTANT: Ensure test data has same factor levels as train data
  for(var in factor_vars) {
    if(is.factor(train_data[[var]])) {
      # Set levels for training data to be all original levels
      levels(train_data[[var]]) <- original_levels[[var]]
      
      # Ensure test data has same levels
      test_data[[var]] <- factor(test_data[[var]], levels = original_levels[[var]])
    }
  }
  
  # Fit model - wrapped in tryCatch to handle any remaining errors gracefully
  mlm_model <- tryCatch({
    lmer(total_ordered_2024 ~   
         customer_age + LOCAL_MARKET_PARTNER + 
         CO2_CUSTOMER + propCases + distance_from_centroid + 
         total_ordered_2023 + (1|Binning_column), 
         data = train_data)
  }, error = function(e) {
    message("Error in fold ", i, ": ", e$message)
    return(NULL)
  })
  
  # Skip this fold if model fitting failed
  if(is.null(mlm_model)) {
    cv_rmse[i] <- NA
    cv_mae[i] <- NA
    next
  }
  
  # Make predictions - with error handling
  predictions <- tryCatch({
    predict(mlm_model, newdata = test_data, allow.new.levels = TRUE)
  }, error = function(e) {
    message("Prediction error in fold ", i, ": ", e$message)
    return(NULL)
  })
  
  # Skip metrics calculation if prediction failed
  if(is.null(predictions)) {
    cv_rmse[i] <- NA
    cv_mae[i] <- NA
    next
  }
  
  # Calculate performance metrics
  cv_rmse[i] <- sqrt(mean((test_data$total_ordered_2024 - predictions)^2, na.rm = TRUE))
  cv_mae[i] <- mean(abs(test_data$total_ordered_2024 - predictions), na.rm = TRUE)
  ss_res <- sum((test_data$total_ordered_2024-predictions)^2, na.rm = TRUE)
  ss_tot <- sum((test_data$total_ordered_2024-mean(test_data$total_ordered_2024, na.rm = TRUE))^2, na.rm = TRUE)
  cv_r2[i] <- 1 - (ss_res/ss_tot)
  
  cat("Fold", i, "completed. RMSE:", cv_rmse[i], "MAE:", cv_mae[i], "\n")
}

# Average performance across folds (ignoring NA values)
mean_rmse <- mean(cv_rmse, na.rm = TRUE)

mean_mae <- mean(cv_mae, na.rm = TRUE)
mean_r2 <- mean(cv_r2, na.rm = TRUE)

cat("\nCross-validation results:\n")
cat("Mean RMSE:", mean_rmse, "\n")
cat("Mean MAE:", mean_mae, "\n")
cat("Individual fold RMSEs:", cv_rmse, "\n")
cat("Individual fold RMSEs:", cv_mae, "\n")
cat("Mean R-squared:", mean_r2, "\n")
cat("Individual fold R-squareds:", cv_r2, "\n")


```
***Predict 2025***
```{r}
ACNR <- Annual_Customer_No_Retailer

final_model <- lmer(total_ordered_2024 ~   
         customer_age + LOCAL_MARKET_PARTNER + 
         CO2_CUSTOMER + propCases + distance_from_centroid + 
         total_ordered_2023 + (1|Binning_column), 
         data = ACNR)

ACNR2025 <- ACNR %>% 
   mutate(total_ordered_2023 = total_ordered_2024,
          customer_age = customer_age +1) %>% 
  select(-total_ordered_2024)


ACNR2025$predictions_2025 <- predict(final_model, newdata = ACNR2025)

ACNR2025$predictions_2025 <- ACNR2025$predictions_2025
```


```{r}
bin_summary <- ACNR2025 %>% 
  mutate(over_400_in_2025 = predictions_2025 > 400) %>% 
  group_by(Binning_column) %>% 
  summarize(
    total_customers = n(),
    ccustomers_over_400_2025 = sum(over_400_in_2025, na.rm = TRUE)  )

print(bin_summary)
```
