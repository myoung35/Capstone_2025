---
title: "Data Source"
author: "Madalyn Young & Imogen Holdsworth"
date: "2025-03-16"
output: 
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes # makes the toc move along
    code_folding: "show"  # Use "hide" to collapse code by default
editor_options: 
  chunk_output_type: inline
---


```{r}
pacman::p_load(tidyverse, scales, dplyr, corrr, janitor, tidyr, psych, readr, lubridate, rpart, rpart.plot, caret, C50, sf, maps, dbscan, geosphere, nnet, randomForest,readxl, tsibble, ggplot2, forecast, tseries, lme4, performance)
```

# Load Orignal Data
```{r}
CustomerProfileData <- read.csv("Data/customer_profile.csv")  
TransactionalData <- read.csv("Data/transactional_data.csv")
AddressZipData <- read.csv("Data/customer_address_and_zip_mapping.csv")
DeliveryCostData <- read.csv("Data/delivery_cost_data.csv")
```

# First Round Cleaning

## Clean Address Data

## Address Data Cleaning
* break address data into seperate columns
* change lat and long to numeric
```{r}
#clean the address data
# Split the column
AddressZipData <- AddressZipData |>
  separate(full.address, into = c("ZIP", "City", "State Name", "State Short", 
                                  "County","Code", "Latitude", "Longitude"), sep = ",")

AddressZipData$Latitude <- as.numeric(AddressZipData$Latitude)

AddressZipData$Longitude <- as.numeric(AddressZipData$Longitude)
```

## Transactional Data Cleaning

```{r}
TransactionalData$TRANSACTION_DATE <- mdy(TransactionalData$TRANSACTION_DATE)
  
TransactionalData <- TransactionalData %>% 
  mutate(Quarter_column = quarter(TRANSACTION_DATE), 
         Quarter_year = paste(Quarter_column, YEAR, sep = " "),
         MONTH = month(TRANSACTION_DATE)) %>% 
  select(-c(LOADED_CASES, DELIVERED_CASES, LOADED_GALLONS, DELIVERED_GALLONS))
```

## Customer Profile Data Cleaning

* Added Entity ID to look at customers with outlet all together
* clean date format
* convert character columns to factors
* convert logical columns to 0/1 - for easier modeling later
* filter out the one customer that has a first delivery date before they were on boarded
```{r}
#clean Customer Profile Data
  CustomerProfileData <-  CustomerProfileData %>% 
  mutate(
    Entity_ID = case_when(
      is.na(PRIMARY_GROUP_NUMBER) ~ CUSTOMER_NUMBER,  # If PRIMARY_GROUP_NUMBER is NA, use CUSTOMER_NUMBER
      TRUE ~ PRIMARY_GROUP_NUMBER),
    ON_BOARDING_DATE = mdy(ON_BOARDING_DATE),
    FIRST_DELIVERY_DATE = mdy(FIRST_DELIVERY_DATE),
    ON_BOARDING_YEAR = year(ON_BOARDING_DATE),
    FIRST_DELIVERY_YEAR = year(FIRST_DELIVERY_DATE))

char_col <- sapply(CustomerProfileData, is.character)
CustomerProfileData[char_col] <- lapply(CustomerProfileData[char_col], as.factor)

logical_cols <- sapply(CustomerProfileData, is.logical)
CustomerProfileData[logical_cols] <- lapply(CustomerProfileData[logical_cols], as.numeric)

#remove the customer where their on_boarding date was first delivery date was before the onboarding date (1 customer)
CustomerProfileData <- CustomerProfileData %>% 
  filter(FIRST_DELIVERY_DATE>=ON_BOARDING_DATE)
```


## Annual Aggregated Transactionl Data

```{r}
#Pivot wide the cost data
#aggregated  transaction data to join to customer table

# aggregate transaction data by customer_number and year
#sum the ordered cases and gallons by customer number and year
#this table is set up so each customer number has  two rows, one for 2023, one for 2024. Each column is sum of ordered cases/loaded cases. delivered cases in that year 
aggregated_cost <- TransactionalData |>
  group_by(CUSTOMER_NUMBER, YEAR) |>
  summarize(orderedCases = sum(ORDERED_CASES),
            orderedGallons = sum(ORDERED_GALLONS))



#The code pivots the database above to have one row per customer and a column for each cases/gallons ordered for each year
aggregated_cost_wide <- aggregated_cost |>
  pivot_wider(
    names_from = YEAR, 
    values_from = c(orderedCases, 
                    orderedGallons),
    names_sep = "_"
  )
```


## Location Clustering

```{r}
CustomerProfile_Location <- CustomerProfileData %>% 
  left_join(AddressZipData, by = c("ZIP_CODE"="zip")) 
```

* do KMeans clustering to identify the four main location clusters
* identify the center (Centroid) of each of the four main clusters
```{r}
#cluster the addresses and calculate the centroid for each cluster
##Multiple centroids
set.seed(123)

kmeans_result <- kmeans(CustomerProfile_Location[,c("Longitude", "Latitude")], centers = 4)

CustomerProfile_Location$cluster <- as.factor(kmeans_result$cluster)


centroids <- CustomerProfile_Location %>% 
  group_by(cluster) %>% 
  summarize(centroid_lon = mean(Longitude), centroid_lat = mean(Latitude))

```


* Calculate the miles distance of each customers location to the centroid
```{r}

haversine_distance <- function(lon1, lat1, lon2, lat2) {
  distHaversine(c(lon1, lat1), c(lon2,lat2))/1609.34 
}# converts meters to miles

#Join main customer data to the clusters created above
CustomerProfile_Location <- CustomerProfile_Location %>% 
  left_join(centroids, by = "cluster")


CustomerProfile_Location <- CustomerProfile_Location %>% 
  mutate(
    distance_to_centroid = mapply(haversine_distance, CustomerProfile_Location$Longitude, CustomerProfile_Location$Latitude, CustomerProfile_Location$centroid_lon, CustomerProfile_Location$centroid_lat)
  )

```


# Annual Customer No Retailer
```{r}
Annual_Customer_No_Retailer <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_wide, by = "CUSTOMER_NUMBER") %>% 
  mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  filter(!(year(FIRST_DELIVERY_DATE) == 2023 & orderedCases_2023 == 0 & orderedGallons_2023 == 0) &
    !(year(FIRST_DELIVERY_DATE) == 2024 & orderedCases_2024 == 0 & orderedGallons_2024 == 0)) %>% 
  filter(!(total_ordered_2023 ==0 & total_ordered_2024 == 0))

```


```{r}
Annual_Customer_No_Retailer <- Annual_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  summarize(FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
            COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
            TRADE_CHANNEL = first(TRADE_CHANNEL),
            SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
            FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
            FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
            ON_BOARDING_DATE = min(ON_BOARDING_DATE),
            ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
            customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            numberOfOutlets = sum(case_when(is.na(PRIMARY_GROUP_NUMBER)~ 1,TRUE ~1)),
            LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
            CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
            
            hasOrderedCases = as.integer(mean(case_when((orderedCases_2023 + orderedCases_2024)>0 ~1, TRUE ~ 0))>0,1,TRUE~0),
            
            propCases = sum(orderedCases_2023, orderedCases_2024)/ sum(total_ordered),
 
            zip_code =  first(ZIP), 
            
            city =  first(City),

            state =   
              first(`State Name`), 

            region = first(cluster),
             
            distance_from_centroid = first(distance_to_centroid),

            total_ordered_2023 = sum(total_ordered_2023),
            total_ordered_2024 = sum(total_ordered_2024),
            percentChangeYOY = ((total_ordered_2024) - (total_ordered_2023))/(total_ordered_2023)) %>% 
  mutate(Binning_column = case_when(
    (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY < 0.10 ~ "low volume low growth",
         (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY >= 0.10 ~ "low volume high growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY < 0.05 ~ "high volume low growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY >=0.05 ~ "high volume high growth",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY > 0 ~ "transtionary growing",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY <= 0 ~ "transitionary declining" )) %>% 
  filter(numberOfOutlets == 1) 

```


# Annual Customer Data Retailer
```{r}
Annual_Customer_Retailer <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_wide, by = "CUSTOMER_NUMBER") %>% 
  mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  filter(!(year(FIRST_DELIVERY_DATE) == 2023 & orderedCases_2023 == 0 & orderedGallons_2023 == 0) &
    !(year(FIRST_DELIVERY_DATE) == 2024 & orderedCases_2024 == 0 & orderedGallons_2024 == 0))
```

```{r}
Annual_Customer_Retailer <- Annual_Customer_Retailer %>% 
  group_by(Entity_ID) %>% 
    mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  group_by(Entity_ID) %>% 
  summarize(FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
            COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
            TRADE_CHANNEL = first(TRADE_CHANNEL),
            SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
            FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
            FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
            ON_BOARDING_DATE = min(ON_BOARDING_DATE),
            ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
            customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            
            LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
            CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
            hasOutlet = first(case_when(is.na(PRIMARY_GROUP_NUMBER)~ 0,TRUE ~1)),
            numberOfOutlets = sum(case_when(is.na(PRIMARY_GROUP_NUMBER)~ 1,TRUE ~1)),
            wellPerformingOutlet = sum(case_when((orderedGallons_2023 + orderedCases_2023) >= 400 ~ 1, (orderedGallons_2024 + orderedCases_2024) >=400 ~ 1, TRUE ~ 0)),
            
            hasOrderedCases = as.integer(mean(case_when((orderedCases_2023 + orderedCases_2024)>0 ~1, TRUE ~ 0))>0,1,TRUE~0),
            
            propCases = sum(orderedCases_2023, orderedCases_2024)/ sum(total_ordered),
            
            GeoSpread = n_distinct(ZIP),
            most_common_zip = if_else(
              numberOfOutlets == 1,  # If only one location, take that ZIP
              first(ZIP),
              ZIP[which.max(tabulate(match(ZIP, unique(ZIP))))]), 
            largest_zip = if_else(
              numberOfOutlets == 1,
              first(ZIP),
              ZIP[which.max(total_ordered)]
            ),
            
            most_common_city = if_else(
              numberOfOutlets == 1,  # If only one location, take that ZIP
              first(City),
              City[which.max(tabulate(match(City, unique(City))))]), 
            largest_city = if_else(
              numberOfOutlets == 1,
              first(City),
              City[which.max(total_ordered)]
            ),
            
            most_common_state = if_else(
              numberOfOutlets == 1,  # If only one location, take that ZIP
              first(`State Name`),
              `State Name`[which.max(tabulate(match(`State Name`, unique(`State Name`))))]), 
            largest_state = if_else(
              numberOfOutlets == 1,
              first(`State Name`),
              `State Name`[which.max(total_ordered)]
            ),
            
            most_common_region = if_else(
              numberOfOutlets == 1,  # If only one location, take that ZIP
              first(cluster),
              cluster[which.max(tabulate(match(cluster, unique(cluster))))]), 
            largest_region = if_else(
              numberOfOutlets == 1,
              first(cluster),
              cluster[which.max(total_ordered)]
            ),
            
            most_common_distance = if_else(
              numberOfOutlets == 1,  # If only one location, take that ZIP
              first(distance_to_centroid),
              distance_to_centroid[which.max(tabulate(match(distance_to_centroid, unique(distance_to_centroid))))]), 
            
            largest_distance = if_else(
              numberOfOutlets == 1,
              first(distance_to_centroid),
              distance_to_centroid[which.max(total_ordered)]
            ),
            
                        
            avg_distance = if_else(
              numberOfOutlets == 1,
              first(distance_to_centroid),
              mean(distance_to_centroid)
            ),
            

            total_ordered_2023 = sum(total_ordered_2023),
            total_ordered_2024 = sum(total_ordered_2024),
            percentChangeYOY = ((total_ordered_2024) - (total_ordered_2023))/(total_ordered_2023))%>% 
  mutate(Binning_column = case_when(
    (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY < 0.10 ~ "low volume low growth",
         (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY > 0.10 ~ "low volume high growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY < 0.05 ~ "high volume low growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY > 0.05 ~ "high volume high growth",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY > 0 ~ "transtionary growing",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY < 0 ~ "transitionary declining" )) %>% 
  filter(numberOfOutlets > 1)
```


#  Unaggregated Dates Customer Data No Retailer

```{r}
aggregated_cost_by_month <- TransactionalData |>
  group_by(CUSTOMER_NUMBER, YEAR, Quarter_column, Quarter_year,MONTH) |>
  summarize(orderedCases = sum(ORDERED_CASES),
            orderedGallons = sum(ORDERED_GALLONS),
            totalOrdered = sum(ORDERED_CASES, ORDERED_GALLONS))
```


```{r}
UnaggregatedDates_Customer_No_Retailer <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_by_month, by = "CUSTOMER_NUMBER") %>% 
   mutate(across(c(orderedCases, orderedGallons, totalOrdered), ~ replace_na(.x, 0))) 
  
```

*create rows for each month
```{r}
all_months <- crossing(
  CUSTOMER_NUMBER = unique(UnaggregatedDates_Customer_No_Retailer$CUSTOMER_NUMBER),
  YEAR = unique(UnaggregatedDates_Customer_No_Retailer$YEAR, na.rm = TRUE),
  MONTH = 1:12
) %>%
  mutate(
    Quarter_column = case_when(
      MONTH >= 1 & MONTH <= 3 ~ 1,
      MONTH >= 4 & MONTH <= 6 ~ 2,
      MONTH >= 7 & MONTH <= 9 ~ 3,
      TRUE ~ 4
    ),
    Quarter_year = paste(Quarter_column, YEAR, sep = " ")
  ) %>% filter(!is.na(YEAR) )

# First, create a customer reference dataset with one row per customer
customer_reference <- UnaggregatedDates_Customer_No_Retailer %>%
  group_by(CUSTOMER_NUMBER) %>%
  summarize(
    PRIMARY_GROUP_NUMBER = first(PRIMARY_GROUP_NUMBER),
    FREQUENT_ORDER_TYPE = first(FREQUENT_ORDER_TYPE),
    FIRST_DELIVERY_DATE = first(FIRST_DELIVERY_DATE),
    ON_BOARDING_DATE = first(ON_BOARDING_DATE),
    COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
    TRADE_CHANNEL = first(TRADE_CHANNEL),
    SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
    LOCAL_MARKET_PARTNER = first(LOCAL_MARKET_PARTNER),
    CO2_CUSTOMER = first(CO2_CUSTOMER),
    ZIP_CODE = first(ZIP_CODE),
    Entity_ID = first(Entity_ID),
    ON_BOARDING_YEAR = first(ON_BOARDING_YEAR),
    FIRST_DELIVERY_YEAR = first(FIRST_DELIVERY_YEAR),
    ZIP = first(ZIP),
    City = first(City),
    `State Name` = first(`State Name`),
    Latitude = first(Latitude),
    Longitude = first(Longitude),
    cluster = first(cluster),
    distance_to_centroid = first(distance_to_centroid)
  )

# Now join the transactions with all_months first, then join with customer reference

UnaggregatedDates_Customer_No_Retailer <- all_months %>%
  left_join(
    UnaggregatedDates_Customer_No_Retailer %>% 
      select(CUSTOMER_NUMBER, YEAR, MONTH, Quarter_column, Quarter_year, orderedCases, orderedGallons, totalOrdered),
    by = c("CUSTOMER_NUMBER", "YEAR", "MONTH")
  ) %>%
  mutate(across(c(orderedCases, orderedGallons, totalOrdered), ~ replace_na(.x, 0))) %>%
  # Join with the customer reference data
  left_join(customer_reference, by = "CUSTOMER_NUMBER")

```


```{r}
BYMONTH_Customer_No_Retailer <- UnaggregatedDates_Customer_No_Retailer %>% 
  filter(is.na(PRIMARY_GROUP_NUMBER)) %>% 
  group_by(Entity_ID, YEAR, MONTH) %>% 
  reframe(
    FREQUENT_ORDER_TYPE = first(FREQUENT_ORDER_TYPE),  # simplified
    COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
    TRADE_CHANNEL = first(TRADE_CHANNEL),
    SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
    FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
    FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
    ON_BOARDING_DATE = min(ON_BOARDING_DATE),
    ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
    customer_age = as.numeric(format(Sys.Date(), "%Y")) - min(ON_BOARDING_YEAR),
            
    LOCAL_MARKET_PARTNER = first(LOCAL_MARKET_PARTNER),  # simplified
    CO2_CUSTOMER = first(CO2_CUSTOMER),  # simplified
    
    hasOrderedCases = as.integer(sum(orderedCases) > 0),
            
    propCases = sum(orderedCases) / sum(totalOrdered),
    
    zip_code = first(ZIP), 
    city = first(City),
    state = first(`State Name`), 
    region = first(cluster),
    distance_from_centroid = first(distance_to_centroid),
    
    orderedCases = sum(orderedCases),
    orderedGallons = sum(orderedGallons),
    totalOrdered = sum(totalOrdered)) %>% 
     mutate(date = as.Date(paste(MONTH, 1, YEAR, sep = "/"), 
                        format="%m/%d/%Y"))

```


Need to replace propcases Nan because there is nothing
Also not sure if because the Q and M/Y columns are seperate how we will do with the time series?
predict total ordered for the next period
create bins column to say if they met target/or our threshbins

```{r}
BYQUARTER_Customer_No_Retailer <- UnaggregatedDates_Customer_No_Retailer %>% 
  filter(is.na(PRIMARY_GROUP_NUMBER)) %>% 
  group_by(Entity_ID, Quarter_year.x) %>% 
  reframe(
    FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
    COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
    TRADE_CHANNEL = first(TRADE_CHANNEL),
    SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
    FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
    FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
    ON_BOARDING_DATE = min(ON_BOARDING_DATE),
    ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
    customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            
    LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
    CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
    
    hasOrderedCases = as.integer(case_when((orderedCases)>0 ~1, TRUE ~ 0)),
            
    propCases = sum(orderedCases)/ sum(totalOrdered),
    
    zip_code =  first(ZIP), 
            
    city =  first(City),

    state =  first(`State Name`), 

    region = first(cluster),
             
    distance_from_centroid = first(distance_to_centroid),
    orderedCases = sum(orderedCases),
    orderedGallons = sum(orderedGallons),
    totalOrdered = sum(totalOrdered)) %>% 
   mutate(date = as.Date(paste0((as.integer(sub(" .*", "", Quarter_year.x)) - 1) * 3 + 1, 
                               "/1/", sub(".* ", "", Quarter_year.x)), 
                        format="%m/%d/%Y")) 
  

```

# Quarter modeling V1 - Nothing here

## Split into Train and Test
```{r}
set.seed(123)

train_set <- BYQUARTER_Customer_No_Retailer %>% 
  filter(Quarter_year.x != '4 2024') %>%
  select(date, totalOrdered)

train_set_extraVariables <- BYQUARTER_Customer_No_Retailer %>% 
  filter(Quarter_year.x != '4 2024')

#12% of the data 
test_set <- BYQUARTER_Customer_No_Retailer %>% 
  filter(Quarter_year.x == '4 2024')


```

```{r}
train_set %>% ggplot()+
  geom_line(aes(x = date, y = totalOrdered))
```

```{r}
adf.test(diff(log(train_set$totalOrdered)), alternative = "stationary", k = 0)
```



```{r}
view(mts)
mts <- ts(train_set$totalOrdered, start = c(2023,1,1), frequency = 4)


model <- auto.arima(mts)

summary(model)
model


forecast(model, 5)
plot(forecast(model,5))



```

```{r}
autoplot(mts)
```


higher sigma^2 is the variance of residuals. higher value means more noise or variability

log liklihood is a measure of how well the model fits the data. less negative would mean a better fit. 

AIC = Akaike information Criterion. this penalizes for having more complexity or more parameters. lower AIC indicate a better fit

BIC = Bayesian information criterion. with stronger penalty for models with more parameters. lower is a better fit

This model sucks and basically tells us we cannot predict the next quarter based on the current information

I want to add in  the other variables to make the model more able to predict by customer

```{r}
#make a matrix for extra variables

xreg <- cbind(
  FREQUENT_ORDER_TYPE = train_set_extraVariables$FREQUENT_ORDER_TYPE,
  COLD_DRINK_CHANNEL = train_set_extraVariables$COLD_DRINK_CHANNEL,
  TRADE_CHANNEL = train_set_extraVariables$TRADE_CHANNEL,
  FIRST_DELIVERY_YEAR = train_set_extraVariables$FIRST_DELIVERY_YEAR,
  ON_BOARDING_YEAR = train_set_extraVariables$ON_BOARDING_YEAR,
  customer_age = train_set_extraVariables$customer_age,
  LOCAL_MARKET_PARTNER = train_set_extraVariables$LOCAL_MARKET_PARTNER,
  CO2_CUSTOMER = train_set_extraVariables$CO2_CUSTOMER,
  hasOrderedCases = train_set_extraVariables$hasOrderedCases,
  propCases = train_set_extraVariables$propCases,
  distance_from_centroid = train_set_extraVariables$distance_from_centroid,
  Q1 = as.numeric(quarter(train_set_extraVariables$date)==1),
  Q2 = as.numeric(quarter(train_set_extraVariables$date)==2),
  Q3 = as.numeric(quarter(train_set_extraVariables$date)==3)
)


```


```{r}
arimax_model <- auto.arima(mts, xreg = xreg)

arimax_model
summary(arimax_model)

```
It does not seem that this model performs any better

I feel like I have no idea what I am doing with the Arima model


# Monthly Modeling

```{r}
BYMONTH_Customer_No_Retailer %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```
```{r}
ts_data <- BYMONTH_Customer_No_Retailer %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYMONTH_Customer_No_Retailer$date)), month(min(BYMONTH_Customer_No_Retailer$date))), 
     frequency = 12) 
```

```{r}
adf.test(ts_data)
```

Data is not stationary. 

```{r}
adf.test(diff(ts_data, differences = 3))
```

difference of 3 is sufficent to make my data stationary
d=1 removes linear trends
d=2 removes quadraitc trends
d = 3 removes cubic trends

a diff of 3 might mean that my data is too seasonal to make. I will try laging 
```{r}
adf.test(diff(ts_data, lag=12))

autoplot(ts_data)

decomposed <- decompose(ts_data, type = "additive")
autoplot(decomposed)

#If I see spikes at lag 12, my data has seasonality
Acf(ts_data, lag.max = 36)
```
```{r}
adf.test(diff(ts_data, lag=14))
```

This gives me stationary data

```{r}
modelSeasonal <- auto.arima(ts_data, seasonal = TRUE)

summary(modelSeasonal)
```

This Arima model is looking a lot better. sigma^2 is lower, log likelihood is less negative. 


Let me check out if removing the over 10K orders 


```{r}
ts_data2 <- BYMONTH_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  mutate(totalOrderedCust = sum(totalOrdered)) %>% 
  filter(totalOrderedCust < 10000) %>% 
  ungroup() %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYMONTH_Customer_No_Retailer$date)), month(min(BYMONTH_Customer_No_Retailer$date))), 
     frequency = 12) 
```

```{r}
adf.test(ts_data2)

#followed same trend as unfiltered data
adf.test(diff(ts_data2, lag = 14))
```

```{r}
modelSeasonal2 <- auto.arima(ts_data2, seasonal = TRUE)

summary(modelSeasonal2)
```
a little better, but really close to the above model. almost like the seasonality is unchanged for large and small customers?

I think I want to do linear regression instead

```{r}
lmModelDATA <- BYMONTH_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  mutate(totalOrderedCust = sum(totalOrdered)) %>% 
  filter(totalOrderedCust < 10000) %>% 
  ungroup() %>% 
  mutate(across(c(propCases), ~ replace_na(.x, 0)))
  

```

```{r}
lm1 <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE+ COLD_DRINK_CHANNEL+ TRADE_CHANNEL+ customer_age+ LOCAL_MARKET_PARTNER+ CO2_CUSTOMER+ propCases+ distance_from_centroid ,lmModelDATA)

summary(lm1)
```

this model actually has a reasonable intercept. a lot of variables are significant. Local market partner is interesting. if they are, then it is a decrease in orders. However, my R2 value is very low

add a timer series object

```{r}
ts_orders <- ts(lmModelDATA$totalOrdered, frequency = 12)
decomposed <- decompose(ts_orders)

lmModelDATA$seasonal_component <- decomposed$seasonal
lmModelDATA$trend_component <- decomposed$trend
```

```{r}
lm2 <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE+ COLD_DRINK_CHANNEL+ TRADE_CHANNEL+ customer_age+ LOCAL_MARKET_PARTNER+ CO2_CUSTOMER+ propCases+ distance_from_centroid + seasonal_component + trend_component ,lmModelDATA)

summary(lm2)

plot(lm2)
```

R value improved significantly with adding the trend components. They are significant, but do not increase order amounts much.

I do not think the intercept makes sense that it starts with negative orders.

I think the biggest things these time series models can tell me is that my customer will order differently in Q1 than they do in Q2


# Quarterly Modeling V2

```{r}
plot_data <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE),
            totalOrderedCases = sum(orderedCases),
            totalOrderedGallons = sum(orderedGallons)) %>%  # Aggregate properly
    pivot_longer(cols = c(totalOrdered, totalOrderedCases, totalOrderedGallons), 
               names_to = "Metric", values_to = "Value") 

BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE),
            totalOrderedCases = sum(orderedCases),
            totalOrderedGallons = sum(orderedGallons)) %>% 
  ggplot(aes(x = date, y = totalOrdered)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  scale_y_continuous(labels = label_number(scale = 1e-6, suffix = "M"))+
  labs(x = NULL, y = NULL, title = "Total Ordered")+
  theme_minimal() +
  theme( 
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank(), 
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)) 

ggsave("seasonality.png", width = 8, height = 6, dpi = 300)

BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE),
            totalOrderedCases = sum(orderedCases),
            totalOrderedGallons = sum(orderedGallons)) %>% 
  ggplot(aes(x = date, y = totalOrderedGallons)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  scale_y_continuous(labels = label_number(scale = 1e-6, suffix = "M"))+
  labs(x = NULL, y = NULL, title = "Total Gallons Ordered")+
  theme_minimal() +
  theme( 
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank(), 
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)) 

ggsave("seasonalitygallons.png", width = 8, height = 6, dpi = 300)
```




```{r}
BYQUARTER_Customer_No_Retailer$quarter <- quarter(BYQUARTER_Customer_No_Retailer$date)
BYQUARTER_Customer_No_Retailer$quarter <- factor(paste0("Q", BYQUARTER_Customer_No_Retailer$quarter))


lmQuarterly <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = BYQUARTER_Customer_No_Retailer)


summary(lmQuarterly)
```

smaller by quarter impact than I would like to see. also low R2

I did not remove the above 10K lets see the difference

```{r}
lmModelDATAQ <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  mutate(totalOrderedCust = sum(totalOrdered)) %>% 
  filter(totalOrderedCust < 10000) %>% 
  ungroup() %>% 
  mutate(across(c(propCases), ~ replace_na(.x, 0)))
```

```{r}
lmQuarterly2 <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = lmModelDATAQ)


summary(lmQuarterly2)
```
my variables are showing more of an impactand I have lots of significance but, my R2 went down when I filtered out high order customers. 


Try squaring some variables (create a curved line relationship instead of linear)
```{r}
lmModelDATAQ$customer_age_squared <- lmModelDATAQ$customer_age^2

lmQuarterly3 <- lm(totalOrdered ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + log(distance_from_centroid + 1) + quarter , data = lmModelDATAQ)


summary(lmQuarterly3)
```

squaring literally brought my R2 down. Logging also did not improve my R2

There is clearly a quarterly trend in ordering I want to capture.  I will try and ARIMA and a regression combined


```{r}
ts_dataQ <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYQUARTER_Customer_No_Retailer$date)), month(min(BYQUARTER_Customer_No_Retailer$date))), 
     frequency = 4) 
```

```{r}
autoplot(ts_dataQ)
```


```{r}
adf.test(ts_dataQ)
```

data is stationary NO differencing!!! this means that the data does not have trends or seasonailty that would make it difficut to predict. What I clearly see from the graph. there is one quarter each year where ordering takes place and we can predict that

```{r}

acf(ts_dataQ) # 1 significant spike q= 1
pacf(ts_dataQ) # 0 significant spike p= 0
QArima <- arima(ts_dataQ, order = c(0,0,1))

summary(QArima)

#the above model is better
QArimaAuto <- auto.arima(ts_dataQ)
summary(QArimaAuto)
```
**for the specified arima**
1. Coefficients:
ma1 (-0.9980): This is the first-order moving average coefficient. A value close to -1 suggests that the series has strong negative autocorrelation at lag 1, meaning that an increase in one period tends to be followed by a decrease in the next.

Intercept (4,369,738.19): This represents the mean level of the series (assuming no differencing was applied).

2. Standard Errors (s.e.):
ma1 standard error (0.4149): Measures the uncertainty around the MA(1) estimate.

Intercept standard error (48,231.29): Suggests that the intercept estimate has a relatively small uncertainty compared to its magnitude.

3. Model Fit Metrics:
sigma² (1.398e+11): This is the estimated variance of the residuals.

Log-likelihood (-115.1): Higher values (less negative) suggest a better fit.

AIC (236.19): A lower AIC indicates a better model fit relative to others.

4. Training Set Error Measures:
ME (-86,206.46): Mean error, showing a slight underestimation bias.

RMSE (373,931.1): Measures how much the predicted values deviate from actual values on average.

MAE (346,158.9): Mean absolute error, showing the typical size of forecast errors.

MPE (-2.70%): Mean percentage error, indicating the direction of bias (slight underestimation).

MAPE (8.11%): Mean absolute percentage error, meaning your model's predictions are, on average, 8.11% off.

MASE (0.5429): Mean absolute scaled error; values below 1 suggest better performance than a naive forecast.

ACF1 (0.1830): Autocorrelation of residuals at lag 1. A low value suggests that residuals are fairly uncorrelated (which is good).

Interpretation:
Your MA(1) coefficient is close to -1, suggesting strong negative autocorrelation.

The AIC is reasonable, though you might want to compare it with other models (e.g., ARIMA(1,0,1), ARIMA(0,1,1)).

The MAPE of 8.11% indicates a fairly good predictive performance.

The residual autocorrelation (ACF1 = 0.183) is low, which means the model does not leave much pattern in the residuals.



okay great, my quarterly arima does look good. but it does not have much complexity by customer (to my understanding) I want to combine it with a regression and see where we get

```{r}
exog_variables  <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(date) %>%
  summarize(
     FREQUENT_ORDER_TYPE = names(which.max(table(FREQUENT_ORDER_TYPE))),  # Select most frequent category
    COLD_DRINK_CHANNEL = names(which.max(table(COLD_DRINK_CHANNEL))),
    TRADE_CHANNEL = names(which.max(table(TRADE_CHANNEL))),
    customer_age = mean(customer_age, na.rm = TRUE),
    LOCAL_MARKET_PARTNER = max(LOCAL_MARKET_PARTNER, na.rm = TRUE),
    CO2_CUSTOMER = max(CO2_CUSTOMER, na.rm = TRUE),
    propCases = mean(propCases, na.rm = TRUE),
    distance_from_centroid = mean(distance_from_centroid, na.rm = TRUE))
```

```{r}
exog_matrix <- exog_variables %>%
  select(-date) %>% 
  as.matrix

arima_with_regression <- arima(ts_dataQ, xreg = exog_matrix, order= c(0,0,1))
```
## Quarterly with customer?
pretty good model here. even better than my Arima quarterly, except this is just for the first customer in my databse

```{r}
# Prepare the data
arima_data <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(date, Entity_ID) %>%
  summarize(
    totalOrdered = sum(totalOrdered, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(Entity_ID, date)

# Create time series for total orders
ts_data <- arima_data %>%
  filter(Entity_ID == first(Entity_ID)) %>%
  pull(totalOrdered) %>%
  ts(start = c(year(min(arima_data$date)), 
               quarter(min(arima_data$date))), 
     frequency = 4)

# Fit basic ARIMA model
arima_model <- auto.arima(diff(ts_data))

# Summary of the model
summary(arima_model)

adf.test(diff(ts_data))
```

I guess I just dont understand how to add in the regression piece because it looks like it has a model for each individual customer, but I cant have that if I doing want over fit. 

## Create a quarterly regression with bin as a predictor
```{r}
BYQUARTER_Customer_No_Retailer <- BYQUARTER_Customer_No_Retailer %>% 
  left_join(Annual_Customer_No_Retailer %>% select(Entity_ID, Binning_column), by = "Entity_ID")
```


Bringing in my binning is actually improving the model. 

```{r}
lmQuarterly2 <- lm((totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER*CO2_CUSTOMER + propCases + distance_from_centroid + quarter +Binning_column , data = BYQUARTER_Customer_No_Retailer)


summary(lmQuarterly2)
```
logging total orders significantly improves my models R2.

This intercept is very good to me at 195 gallons

### Quarterly Log Regression 0.52 R2
```{r}
lmQuarterly3 <- lm(log(totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER*CO2_CUSTOMER + propCases + distance_from_centroid + quarter +Binning_column , data = BYQUARTER_Customer_No_Retailer)


summary(lmQuarterly3)
```
I want to now just do more data analysis in the bins

- Does the seasonality hold true in all buckets?
- How many local market partners are in low growth, low volume buckets
  - proportion that order CO2 compared to high performing local market partners
 
# Bin Data Analysis  
```{r}
ByQuarter2 <- BYQUARTER_Customer_No_Retailer %>% 
  group_by(Entity_ID) %>% 
  mutate(totalOrderedCust = sum(totalOrdered)) %>% 
  filter(totalOrderedCust != 0) %>% 
  filter(!is.na(Binning_column)) %>% #These are the customers with a first delivery date but no orders 
  ungroup()

unique(ByQuarter2$Binning_column)
```

## Low Volume Low Growth
```{r}
ByQuarterLVLG <-  ByQuarter2 %>% 
  filter(Binning_column == 'low volume low growth')
```

### LVLG Trend line  
```{r}
ByQuarterLVLG %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```
Interesting trend for low volume low growth customers. We still see a higher order amount in  Q2 in both years. Except it almost seems like they are just not ordering any more. Like the spike is minimal the drop off is steep

low growth is anything less than 10% 

I wonder how many customers are low volume and have negative growth
There are 6,541 customers that are LVLG and have a negative percentage change
```{r}
Annual_Customer_No_Retailer %>% 
  filter(Binning_column == 'low volume low growth' & percentChangeYOY < 0) %>% 
  pull(percentChangeYOY) %>% 
  mean()

```
Average low growth is a decline of 38.5% 
Median is a decline of 30.3%
### LVLG CO2/LPM analysis  
```{r}
ByQuarterLVLG %>% 
  group_by(LOCAL_MARKET_PARTNER, CO2_CUSTOMER) %>% 
  summarise(unique_count = n_distinct(Entity_ID), .groups = "drop") %>% 
  mutate(proportion = unique_count / sum(unique_count)) 
```
  
  wow so many LVLG customers that are lpm. 97% of this group is LMP. 89% of the full data set are LMP. so this group does show additional heaviness. 
  
CO2 customers is a 50/50 split in LVLG customers. So we have a higher proportion of the full CO2 customers database that are LVLG, but a lower proportion  of them are local market partners. Which is a hypothetically good thing right, like if we want our LMP to be well performing and be ordering CO2, we at least see that there are less LMP's that do order CO2 are LVLG (50% vs 88%) We have more non local market partners that order CO2 in this bin

### LVLG Cases versus gallons
I wonder if we see a decline in cases more than gallons in this group  
```{r}
Annual_Customer_No_Retailer %>% 
  filter(Binning_column == 'low volume low growth' & percentChangeYOY < 0) %>% 
  ggplot()+
  geom_point(aes(x=propCases, y = percentChangeYOY))
```
There does seem to be a cluster of lower precentchange YOY and a lower proportion of cases ordered. So are customers that order cases more volitile than customers that order gallons?

### LVLG On boarding year
I want to see the most common on boarding year or customer age in this group  
```{r}
ByQuarterLVLG %>% 
  distinct(Entity_ID, customer_age) %>% 
  ggplot(aes(x=customer_age)) +
  geom_histogram() +
  theme_minimal()

ByQuarterLVLG %>% 
  distinct(Entity_ID, ON_BOARDING_YEAR) %>% 
  ggplot(aes(x=factor(ON_BOARDING_YEAR))) +
  geom_bar() +
  theme_minimal()
```
On average these customers are 9.6 years old. the median is 7. it looks like most are young customers though, from 2022. Actually a really interesting spike there. I wonder if most 2022 customers are declining
  
How well can we predict this data?
```{r regression no log}

lmQuarterlyLVLG <- lm((totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = ByQuarterLVLG)


summary(lmQuarterlyLVLG)
```
  No this is really bad. Logging it does not really improve my model. Interesting that we lose the ability to predict with more segmentation in this group. 
  
```{r regression with log}

lmQuarterlyLVLG2 <- lm(log(totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = ByQuarterLVLG)


summary(lmQuarterlyLVLG2)
```
  
## Low Volume High Growth


```{r}
ByQuarterLVHG <-  ByQuarter2 %>% 
  filter(Binning_column == 'low volume high growth')
```
  
```{r}
ByQuarterLVHG %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```
okay we would expect such a high spike in this group. It looks like there is the trend of more orders in Q3 in both years as well. Although we do see high order amounts in Q1 and Q2 as well. Almost like this group is ramping up. Maybe it will level out eventually?

I wonder if more of this group has a sooner on boarded date and that could be why we see such large growth in 2024?

### LVHG  Onboarding year
```{r}
ByQuarterLVHG %>% 
  distinct(Entity_ID, ON_BOARDING_YEAR) %>% 
  ggplot(aes(x=factor(ON_BOARDING_YEAR))) +
  geom_bar() +
  theme_minimal()
```
okay yeah, so many of these customers were onboarded in the last year or two. Which means all those 2024 are shoing infinte growth. 

I want to see the trend line without these 2024 customers

```{r}
ByQuarterLVHG %>% 
  filter(ON_BOARDING_YEAR <= 2022) %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

Actually the customers with onboarding date less than 2023 have a higher cumulative order volume in Q3. I do see a huge dip in Q4. I do see the same Q3 trend. But the spike is impressive for this group still. Definitly something here in that these custoemrs are growing for some reason. I wonder which on boarding year customers are growing the most?

I wonder if we can determine how soon after their onboarding date they will be delivered to? because it seems like from this graph that customers on boarded right still wait until Q3 for delivery
```{r}
ByQuarterLVHG %>% 
  filter(ON_BOARDING_YEAR == 2024) %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  them
```


```{r}
ByQuarterLVHG %>% 
  filter(ON_BOARDING_YEAR<=2022) %>% 
  group_by(ON_BOARDING_YEAR) %>% 
  summarise(unique_count = n_distinct(Entity_ID)) %>% 
  arrange(unique_count)
  

```

just an increasing trend I guess. The older the customer the less likely they are to be a low volumne, high growth customer. makes me think customers are only low volume for x number of years. I would be curious to see more historical data on these old old customers that are growing significantly yoy are are still lv. did they ever have high volume? did they experience a lapse? do some customers just only order sparadoicly? 

```{r}
Annual_Customer_No_Retailer %>% 
  filter(Binning_column == 'low volume high growth' & ON_BOARDING_YEAR<=2022 & percentChangeYOY != Inf) %>% 
  pull(percentChangeYOY) %>% 
  mean()

```
Wow average growth is 92% excluding Inf growth customers AND customers less than equal to 2022. Median is lower at 36%. Still really high growth in this group of customers

### LVHG CO2/LPM analysis  
```{r}
ByQuarterLVHG %>% 
  group_by(LOCAL_MARKET_PARTNER, CO2_CUSTOMER) %>% 
  summarise(unique_count = n_distinct(Entity_ID), .groups = "drop") %>% 
  mutate(proportion = unique_count / sum(unique_count)) 
```
98.4% of this group are local market partners. Again seeing a higher proprtion of LPM's under performing. Seeing that we are capturing the LMP in the low performing groups that are NOT CO2 customers. Most CO2 customers are LMPs, so in both this group and LVLG group we are seeing an even proportion of LMP that are and are not CO2. Which means potentially that that LMP that have CO2 are the high performing LMP

### LVHG Cases versus gallons
I wonder if we see a decline in cases more than gallons in this group  
```{r}
Annual_Customer_No_Retailer %>% 
  filter(Binning_column == 'low volume high growth' & ON_BOARDING_YEAR<=2022) %>% 
  ggplot()+
  geom_point(aes(x=propCases, y = percentChangeYOY))
```
No correlation with growth and number of cases ordered it looks like. 

















  
```{r}
CustomerProfileData %>% 
  count(LOCAL_MARKET_PARTNER, CO2_CUSTOMER) %>% 
  mutate(proportion = n / sum(n))
```
88% of full database are both CO2 customers and LMPs. So this means that most CO2 customers are LMP
39% of full database are CO2 customers
89% of full database are LMP

```{r}
CustomerProfileData %>% 
  filter(year(ON_BOARDING_DATE)==2022) %>% 
  count(n_distinct(Entity_ID))
```


### LVHG Arima modeling
```{r}
ts_dataLVHG <- ByQuarterLVHG %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYQUARTER_Customer_No_Retailer$date)), month(min(BYQUARTER_Customer_No_Retailer$date))), 
     frequency = 4) 
```

```{r}
autoplot(ts_dataLVHG)
```


```{r}
adf.test(ts_dataLVHG)
```

data is not stationary need to difference

```{r}
adf.test(diff(ts_dataLVHG, differences = 1))
```

differencing made it worse. I wonder if it is because of the cusotmers with no order history... Also cant do 2, or 3, I get NA. So that means I cant do an arima model without further transformation

### Regression 
```{r regression no log}

lmQuarterlyLVHG <- lm(log(totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = ByQuarterLVHG)


summary(lmQuarterlyLVHG)
```

log and no log this model is also pathetic on its own


## High Volume High Growth Customers


```{r}
ByQuarterHVHG <-  ByQuarter2 %>% 
  filter(Binning_column == 'high volume high growth')
```

### Seasonality graph 
```{r}
ByQuarterHVHG %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

HVHG shows trend in Q3 really clearly

### On boarding year
```{r}
ByQuarterHVHG %>% 
  distinct(Entity_ID, ON_BOARDING_YEAR) %>% 
  ggplot(aes(x=factor(ON_BOARDING_YEAR))) +
  geom_bar() +
  theme_minimal()
```

### HVHG CO2/LPM analysis  
```{r}
ByQuarterHVHG %>% 
  group_by(LOCAL_MARKET_PARTNER, CO2_CUSTOMER) %>% 
  summarise(unique_count = n_distinct(Entity_ID), .groups = "drop") %>% 
  mutate(proportion = unique_count / sum(unique_count)) 
```
Finally seeing some seperation. More customers in this bucket are both LMP and CO2 

### Cases 
```{r}
Annual_Customer_No_Retailer %>% 
  filter(Binning_column == 'high volume high growth') %>% 
  ggplot()+
  geom_point(aes(x=propCases, y = percentChangeYOY))
```

Again not a huge trend of ordering cases compared to the % growth yoy. I do see more higher % growth associated with more cases generally. And there is that one outlier. This is interesting because it is against the overall trend I have seen. I wonder what the coef would be in a regression model


The coef is positive
Also interesting hat the R2 became very high for this group (comparitively). It is interesting the R2 reflects the total data model I built
```{r regression no log}

lmQuarterlyHVHG <- lm((totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = ByQuarterHVHG)


summary(lmQuarterlyHVHG)
```

### HVHG Arima modeling
```{r}
ts_dataHVHG <- ByQuarterHVHG %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYQUARTER_Customer_No_Retailer$date)), month(min(BYQUARTER_Customer_No_Retailer$date))), 
     frequency = 4) 
```

```{r}
adf.test(ts_dataHVHG)
```


This is the data that is stationary. I think this is the group that is driving all my other models

```{r}
acf(ts_dataHVHG) # 1 significant spike q= 1
pacf(ts_dataHVHG) # 0 significant spike p= 0
QArimaHVHG <- arima(ts_dataHVHG, order = c(0,0,1))

summary(QArimaHVHG)
```

not a great model based on the sigma, but okay

## High volume Low Growth
```{r}
ByQuarterHVLG <-  ByQuarter2 %>% 
  filter(Binning_column == 'high volume low growth')
```

### Seasonality graph 
```{r}
ByQuarterHVLG %>% 
  group_by(date) %>% 
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%  # Aggregate properly
  ggplot(aes(x = date, y = totalOrdered, group = 1)) +
  geom_line() +  # Connect points with a line
  geom_point() +  # Show actual data points
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

We see the Q2/Q3 trend in this group..
 interesting to see the drop off even though they are still well above threshold. especially in Q1 and Q4
 
### On boarding year
```{r}
ByQuarterHVLG %>% 
  distinct(Entity_ID, ON_BOARDING_YEAR) %>% 
  ggplot(aes(x=factor(ON_BOARDING_YEAR))) +
  geom_bar() +
  theme_minimal()
```

2022 again. 2019 interestingly is also prominant in this group



### HVLG CO2/LPM analysis  
```{r}
ByQuarterHVLG %>% 
  group_by(LOCAL_MARKET_PARTNER, CO2_CUSTOMER) %>% 
  summarise(unique_count = n_distinct(Entity_ID), .groups = "drop") %>% 
  mutate(proportion = unique_count / sum(unique_count)) 
```

Again I see the segregation in proportion in high performing customers that are both LMP and have CO2


### regression

again this high performing group is really showing it is carrying the model predictions. logging makes it worse

Q4 is not a significant predictor, only Q3 is at the 99
```{r regression no log}

lmQuarterlyHVLG <- lm((totalOrdered) ~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL + TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + quarter , data = ByQuarterHVLG)


summary(lmQuarterlyHVLG)
```

### HVHG Arima modeling
```{r}
ts_dataHVLG <- ByQuarterHVLG %>% 
  group_by(date) %>%
  summarize(totalOrdered = sum(totalOrdered, na.rm = TRUE)) %>%
  arrange(date) %>%  # Ensure data is ordered by time
  pull(totalOrdered) %>%  # Extract the numeric vector
  ts(start = c(year(min(BYQUARTER_Customer_No_Retailer$date)), month(min(BYQUARTER_Customer_No_Retailer$date))), 
     frequency = 4) 
```

```{r}
adf.test(ts_dataHVLG)
```

This is the data that is stationary. I think this is the group that is driving all my other models

```{r}
acf(ts_dataHVLG) # 1 significant spike q= 1
pacf(ts_dataHVLG) # 0 significant spike p= 0
QArimaHVLG <- arima(ts_dataHVLG, order = c(0,0,1))

summary(QArimaHVLG)
```

Another bad arima model go figure

## Multi Level Modeling
```{r}
MLM1 <- lmer(total_ordered_2024~ FREQUENT_ORDER_TYPE + COLD_DRINK_CHANNEL+ TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + (1|Binning_column), data = Annual_Customer_No_Retailer)

#1| means that I expect the predictors to have the same relationship for each group. If I thinkk that it might be different I can put it in that spot instead
```

```{r}
summary(MLM1)

AIC(MLM1)
BIC(MLM1)

#if this is near 0 a MLM model is good. My output shows it is good
VarCorr(MLM1)

#ICC > 0.1 -> group effects matter and MLM is justified
#adust IIC is greater the 0.1, but unadjusted is not
icc(MLM1)
```
```{r}
fixef(MLM1) #population level estimates
ranef(MLM1)
```
the intercepts are not looking good 

## Split into Train and Test

```{r}
Annual_Customer_No_Retailer$Binning_column <- as.factor(Annual_Customer_No_Retailer$Binning_column)

# Then split into train_setMLM and test_setMLM
set.seed(123)

# you enter the proportion for the split here. I'd suggest .8
inTrainMLM <- createDataPartition(Annual_Customer_No_Retailer$total_ordered_2024, p=.75, list=FALSE)

# use the row indexes from line 87 to create the 2 sets.
# train includes the index, test excludes the index.

train_setMLM <- Annual_Customer_No_Retailer[inTrainMLM,]
test_setMLM <- Annual_Customer_No_Retailer[-inTrainMLM,]

```

```{r}
unique(train_setMLM$FREQUENT_ORDER_TYPE)
```

```{r}
unique(test_setMLM$FREQUENT_ORDER_TYPE)
```


```{r}
MLMTrain <- lmer(total_ordered_2024~ FREQUENT_ORDER_TYPE+ COLD_DRINK_CHANNEL+ TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + (1|Binning_column), data = train_setMLM)

```

```{r}
summary(MLMTrain)
```
```{r}
test_setMLM$predictions <- predict(MLMTrain, newdata =test_setMLM, allow.new.levels = TRUE)
```


```{r}
rmse(test_setMLM$total_ordered_2024, test_setMLM$predictions)
```

```{r}
rsq <- cor(test_setMLM$total_ordered_2024, test_setMLM$predictions)^2

print(rsq)
```

```{r}

ggplot(test_setMLM, aes(x = total_ordered_2024, y = predictions)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs. Predicted", x = "Actual", y = "Predicted") +
  theme_minimal()

```

```{r}
Annual_Customer_No_Retailer2 <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_wide, by = "CUSTOMER_NUMBER") %>% 
  mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  filter(!(year(FIRST_DELIVERY_DATE) == 2023 & orderedCases_2023 == 0 & orderedGallons_2023 == 0) &
    !(year(FIRST_DELIVERY_DATE) == 2024 & orderedCases_2024 == 0 & orderedGallons_2024 == 0)) %>% 
  filter(!(total_ordered_2023 ==0 & total_ordered_2024 == 0))

```


```{r}
Annual_Customer_No_Retailer2 <- Annual_Customer_No_Retailer2 %>% 
  group_by(Entity_ID) %>% 
  summarize(FREQUENT_ORDER_TYPE = FREQUENT_ORDER_TYPE[which.max(tabulate(match(FREQUENT_ORDER_TYPE, unique(FREQUENT_ORDER_TYPE))))],
            COLD_DRINK_CHANNEL = first(COLD_DRINK_CHANNEL),
            TRADE_CHANNEL = first(TRADE_CHANNEL),
            SUB_TRADE_CHANNEL = first(SUB_TRADE_CHANNEL),
            FIRST_DELIVERY_DATE = min(FIRST_DELIVERY_DATE),
            FIRST_DELIVERY_YEAR = min(FIRST_DELIVERY_YEAR),
            ON_BOARDING_DATE = min(ON_BOARDING_DATE),
            ON_BOARDING_YEAR = min(ON_BOARDING_YEAR),
            customer_age = as.numeric(format(Sys.Date(), "%Y")) - ON_BOARDING_YEAR,
            numberOfOutlets = sum(case_when(is.na(PRIMARY_GROUP_NUMBER)~ 1,TRUE ~1)),
            LOCAL_MARKET_PARTNER = LOCAL_MARKET_PARTNER[which.max(tabulate(match(LOCAL_MARKET_PARTNER,unique(LOCAL_MARKET_PARTNER))))],
            CO2_CUSTOMER =  CO2_CUSTOMER[which.max(tabulate(match(CO2_CUSTOMER, unique(CO2_CUSTOMER))))],
            
            hasOrderedCases = as.integer(mean(case_when((orderedCases_2023 + orderedCases_2024)>0 ~1, TRUE ~ 0))>0,1,TRUE~0),
            
            propCases = sum(orderedCases_2023, orderedCases_2024)/ sum(total_ordered),
 
            zip_code =  first(ZIP), 
            
            city =  first(City),

            state =   
              first(`State Name`), 

            region = first(cluster),
             
            distance_from_centroid = first(distance_to_centroid),

            total_ordered_2023 = case_when(sum(total_ordered_2023)==0~1,TRUE~sum(total_ordered_2023)),
            total_ordered_2024 = sum(total_ordered_2024),
            percentChangeYOY = ((total_ordered_2024) - (total_ordered_2023))/(total_ordered_2023)) %>% 
  mutate(Binning_column = case_when(
    (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY < 0.10 ~ "low volume low growth",
         (total_ordered_2023 < 400 & total_ordered_2024 < 400) & percentChangeYOY >= 0.10 ~ "low volume high growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY < 0.05 ~ "high volume low growth",
         (total_ordered_2023 > 400 & total_ordered_2024 > 400) & percentChangeYOY >=0.05 ~ "high volume high growth",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY > 0 ~ "transtionary growing",
         (total_ordered_2023 >= 400 | total_ordered_2024 >= 400) & percentChangeYOY <= 0 ~ "transitionary declining" )) %>% 
  filter(numberOfOutlets == 1) 

```

```{r}
Annual_Customer_No_Retailer2$Binning_column <- as.factor(Annual_Customer_No_Retailer2$Binning_column)

# Then split into train_setMLM and test_setMLM
set.seed(123)

# you enter the proportion for the split here. I'd suggest .8
inTrainMLM2 <- createDataPartition(Annual_Customer_No_Retailer2$total_ordered_2024, p=.7, list=FALSE)

# use the row indexes from line 87 to create the 2 sets.
# train includes the index, test excludes the index.

train_setMLM2 <- Annual_Customer_No_Retailer2[inTrainMLM2,]
test_setMLM2 <- Annual_Customer_No_Retailer2[-inTrainMLM2,]

```



```{r}
MLMTrain2 <- lmer(total_ordered_2024~  COLD_DRINK_CHANNEL+ TRADE_CHANNEL + customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + (1|Binning_column), data = train_setMLM2)

```



```{r}
test_setMLM2$predictions <- predict(MLMTrain2, newdata =test_setMLM2, allow.new.levels = TRUE)
```


```{r}
rmse(test_setMLM2$total_ordered_2024, test_setMLM2$predictions)
```
rmse barely lower and rsq basically the same with this change
```{r}
rsq <- cor(test_setMLM2$total_ordered_2024, test_setMLM2$predictions)^2

print(rsq)
```

```{r}
ggplot(test_setMLM2, aes(x = total_ordered_2024, y = predictions)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs. Predicted", x = "Actual", y = "Predicted") +
  theme_minimal()
```

## MLM with percent change added in
**The concern with this model is that the bin column is based on the % change YOY, which is a factor of what we are trying to predict. But I will try the model, because we can recommend they replace the % change yoy with CAGR with more data

```{r}

set.seed(123)

# you enter the proportion for the split here. I'd suggest .8
inTrainMLM3<- createDataPartition(Annual_Customer_No_Retailer2$total_ordered_2024, p=.8, list=FALSE)

# use the row indexes from line 87 to create the 2 sets.
# train includes the index, test excludes the index.

train_setMLM3 <- Annual_Customer_No_Retailer2[inTrainMLM3,]
test_setMLM3 <- Annual_Customer_No_Retailer2[-inTrainMLM3,]

```


```{r}
MLMTrain3 <- lmer(total_ordered_2024~customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + percentChangeYOY +  (1|Binning_column), data = train_setMLM3)

```

$$
\hat{Y}_{2024Orders} = \beta_0 + \beta_1{customerAge} + \beta_2{LMP} + \beta_3{CO2} + \beta_4{propcases} +  \beta_4{distance} + \beta_4{2023Orders} + \beta_4{YOYDelta} + \beta_5{1|Bin} 
$$





```{r}
summary(MLMTrain3)
```

```{r}
unique(train_setMLM3$Binning_column)
```

```{r}
unique(test_setMLM3$Binning_column)
```



```{r}
test_setMLM3$predictions <- predict(MLMTrain3, newdata =test_setMLM3, allow.new.levels = TRUE)
```


```{r}
rmse(test_setMLM3$total_ordered_2024, test_setMLM3$predictions)
```

```{r}
rsq <- cor(test_setMLM3$total_ordered_2024, test_setMLM3$predictions)^2

print(rsq)
```
fluctuates between .74 and .83 R2 value based off the train test split. 
Removed the factor variables and saw no noticable difference

```{r}
ggplot(test_setMLM3, aes(x = total_ordered_2024, y = predictions)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  labs(title = "Actual vs. Predicted", x = "Actual", y = "Predicted") +
  scale_y_continuous(labels = label_number(scale = 1e-3, suffix = "K"))+
  scale_x_continuous(labels = label_number(scale = 1e-3, suffix = "K"))+
  theme_minimal()+
   theme( 
    panel.grid = element_blank(), 
    axis.line = element_line(color = "black"),
    panel.background = element_rect(fill = "white", color = NA),
    plot.background = element_rect(fill = "white", color = NA)) 
  

ggsave("MLMpredVsActual.png", width = 8, height = 6, dpi = 300)
```

This model improves slightly. Does seem to be more clustered around the center line. Can we conclude CAGR would be a good metric to add in? I think likely. I am already including order amounts

```{r}
car::vif(MLMTrain3)
```
vif of 5 means there is no signficant multicollinearity.

I guess what this means is that orders and % change are so variable that the bins themselves dont capture the correlation very heavily 

"This means keeping both variables in the model could still be informative—one capturing broad customer segmentation and the other providing a more continuous measure of change. Sounds like a solid approach! Let me know if you need help interpreting the results."

##cross validation instead of train test

```{r}
# First, ensure all factors have consistent levels throughout your dataset
factor_vars <- c("FREQUENT_ORDER_TYPE", "COLD_DRINK_CHANNEL", "TRADE_CHANNEL", "Binning_column")

for(var in factor_vars) {
  if(is.factor(Annual_Customer_No_Retailer2[[var]]) || is.character(Annual_Customer_No_Retailer2[[var]])) {
    Annual_Customer_No_Retailer2[[var]] <- factor(Annual_Customer_No_Retailer2[[var]])
    print(paste("Factor", var, "has", length(levels(Annual_Customer_No_Retailer2[[var]])), "levels"))
  }
}

# Store original factor levels
original_levels <- list()
for(var in factor_vars) {
  if(is.factor(Annual_Customer_No_Retailer2[[var]])) {
    original_levels[[var]] <- levels(Annual_Customer_No_Retailer2[[var]])
  }
}
```



```{r}
Annual_Customer_No_Retailer2 <- Annual_Customer_No_Retailer2 %>% 
  filter(Entity_ID != 4445)
set.seed(1234)
folds <- createFolds(unique(Annual_Customer_No_Retailer2$total_ordered_2024), k = 5, list = TRUE, returnTrain = FALSE)

# Initialize storage for performance metrics
cv_rmse <- numeric(length(folds))
cv_mae <- numeric(length(folds))
cv_r2 <- numeric(length(folds))

```


```{r}
for(i in seq_along(folds)) {
  # Create test set indices
  
  test_indices <- folds[[i]]
  
  # Train and test sets
  train_data <- Annual_Customer_No_Retailer2[-test_indices, ]
  test_data <- Annual_Customer_No_Retailer2[test_indices, ]
  
  # IMPORTANT: Ensure test data has same factor levels as train data
  for(var in factor_vars) {
    if(is.factor(train_data[[var]])) {
      # Set levels for training data to be all original levels
      levels(train_data[[var]]) <- original_levels[[var]]
      
      # Ensure test data has same levels
      test_data[[var]] <- factor(test_data[[var]], levels = original_levels[[var]])
    }
  }
  
  # Fit model - wrapped in tryCatch to handle any remaining errors gracefully
  mlm_model <- tryCatch({
    lmer(total_ordered_2024 ~   
         customer_age + LOCAL_MARKET_PARTNER + 
         CO2_CUSTOMER + propCases + distance_from_centroid + 
         total_ordered_2023 + (1|Binning_column), 
         data = train_data)
  }, error = function(e) {
    message("Error in fold ", i, ": ", e$message)
    return(NULL)
  })
  
  # Skip this fold if model fitting failed
  if(is.null(mlm_model)) {
    cv_rmse[i] <- NA
    cv_mae[i] <- NA
    next
  }
  
  # Make predictions - with error handling
  predictions <- tryCatch({
    predict(mlm_model, newdata = test_data, allow.new.levels = TRUE)
  }, error = function(e) {
    message("Prediction error in fold ", i, ": ", e$message)
    return(NULL)
  })
  
  # Skip metrics calculation if prediction failed
  if(is.null(predictions)) {
    cv_rmse[i] <- NA
    cv_mae[i] <- NA
    next
  }
  
  # Calculate performance metrics
  cv_rmse[i] <- sqrt(mean((test_data$total_ordered_2024 - predictions)^2, na.rm = TRUE))
  cv_mae[i] <- mean(abs(test_data$total_ordered_2024 - predictions), na.rm = TRUE)
  ss_res <- sum((test_data$total_ordered_2024-predictions)^2, na.rm = TRUE)
  ss_tot <- sum((test_data$total_ordered_2024-mean(test_data$total_ordered_2024, na.rm = TRUE))^2, na.rm = TRUE)
  cv_r2[i] <- 1 - (ss_res/ss_tot)
  
  cat("Fold", i, "completed. RMSE:", cv_rmse[i], "MAE:", cv_mae[i], "\n")
}

# Average performance across folds (ignoring NA values)
mean_rmse <- mean(cv_rmse, na.rm = TRUE)

mean_mae <- mean(cv_mae, na.rm = TRUE)
mean_r2 <- mean(cv_r2, na.rm = TRUE)

cat("\nCross-validation results:\n")
cat("Mean RMSE:", mean_rmse, "\n")
cat("Mean MAE:", mean_mae, "\n")
cat("Individual fold RMSEs:", cv_rmse, "\n")
cat("Individual fold RMSEs:", cv_mae, "\n")
cat("Mean R-squared:", mean_r2, "\n")
cat("Individual fold R-squareds:", cv_r2, "\n")


```

# Examine the data in fold 1
```{r}
test_data_fold2 <- Annual_Customer_No_Retailer2[folds[[1]], ]

# Look at the distribution of key variables
summary(test_data_fold2$total_ordered_2024)
summary(test_data_fold2$total_ordered_2023)
table(test_data_fold1$Binning_column)
```
```{r}
# Plot actual vs predicted for fold 1
predictions <- predict(mlm_model, newdata = test_data_fold2, allow.new.levels = TRUE)
plot(test_data_fold2$total_ordered_2024, predictions, 
     main="Fold 1: Actual vs Predicted",
     xlab="Actual", ylab="Predicted")
abline(0, 1, col="red")  # Ideal prediction line
```

```{r}
errors <- abs(test_data_fold1$total_ordered_2024 - predictions)
worst_predictions <- order(errors, decreasing=TRUE)[1:5]
test_data_fold1[worst_predictions, ]
```
### The first fold is consistently poor. I wonder if I filter out customers ordering more than 50K gallons if that improves performance?It looks like my outlier driving performance is a customer that ordered 70K in 2024

Wow year filtering out that one customer 4445 improved my model. All folds are now a positve R and the average R2 is 0.77


## Predict 2025
```{r}
ACNR <- Annual_Customer_No_Retailer2

final_model <- lmer(total_ordered_2024 ~   
         customer_age + LOCAL_MARKET_PARTNER + 
         CO2_CUSTOMER + propCases + distance_from_centroid + 
         total_ordered_2023 + (1|Binning_column), 
         data = ACNR)

ACNR2025 <- ACNR %>% 
   mutate(total_ordered_2023 = total_ordered_2024) %>% 
  select(-total_ordered_2024)


ACNR2025$predictions_2025 <- predict(final_model, newdata = ACNR2025)

ACNR2025$predictions_2025 <- ACNR2025$predictions_2025
```


```{r}
bin_summary <- ACNR2025 %>% 
  mutate(over_400_in_2025 = predictions_2025 > 400) %>% 
  group_by(Binning_column) %>% 
  summarize(
    total_customers = n(),
    ccustomers_over_400_2025 = sum(over_400_in_2025, na.rm = TRUE)  )

print(bin_summary)
```
I got 44 customers that are now over 400 predicted 




## MLM with customers with less that 400 gallons in 2023
```{r}
LessThan400 <- Annual_Customer_No_Retailer2 %>% 
  filter(total_ordered_2023 <=400)
set.seed(123)

# you enter the proportion for the split here. I'd suggest .8
inTrainMLM3<- createDataPartition(LessThan400$total_ordered_2024, p=.8, list=FALSE)

# use the row indexes from line 87 to create the 2 sets.
# train includes the index, test excludes the index.

train_setMLM3 <- LessThan400[inTrainMLM3,]
test_setMLM3 <- LessThan400[-inTrainMLM3,]

```


```{r}
MLMTrain3 <- lmer(total_ordered_2024~customer_age + LOCAL_MARKET_PARTNER + CO2_CUSTOMER + propCases + distance_from_centroid + total_ordered_2023 + percentChangeYOY +  (1|Binning_column), data = train_setMLM3)

```

```{r}
test_setMLM3$predictions <- predict(MLMTrain3, newdata =test_setMLM3, allow.new.levels = TRUE)
```


```{r}
rmse(test_setMLM3$total_ordered_2024, test_setMLM3$predictions)
```

```{r}
rsq <- cor(test_setMLM3$total_ordered_2024, test_setMLM3$predictions)^2

print(rsq)
```






## LMP t-test



```{r}
# the "main" data that was used previously : all customer data profile joined with transaction using the Customer Profile Location Data created above.

Customer_Full_Data <- CustomerProfile_Location %>% 
  left_join(aggregated_cost_wide, by = "CUSTOMER_NUMBER") %>% 
  mutate(across(c(orderedCases_2023, orderedCases_2024, orderedGallons_2023, orderedGallons_2024), ~ replace_na(.x, 0)),
         total_ordered = (orderedCases_2024 + orderedGallons_2024 +orderedCases_2023 + orderedGallons_2023),
         total_ordered_2023 = (orderedCases_2023 + orderedGallons_2023),
         total_ordered_2024 = (orderedCases_2024 + orderedGallons_2024)) %>% 
  filter(!(year(FIRST_DELIVERY_DATE) == 2023 & orderedCases_2023 == 0 & orderedGallons_2023 == 0) &
    !(year(FIRST_DELIVERY_DATE) == 2024 & orderedCases_2024 == 0 & orderedGallons_2024 == 0)) %>% 
  filter(!(total_ordered_2023 ==0 & total_ordered_2024 == 0))

Customer_Full_Data <- Customer_Full_Data %>%
  mutate(has_outlet = if_else(!is.na(PRIMARY_GROUP_NUMBER), 1, 0))

outlet_counts <- Customer_Full_Data %>%
  group_by(Entity_ID) %>%
  summarise(number_of_outlets = n_distinct(CUSTOMER_NUMBER), .groups = "drop")

Customer_Full_Data <- Customer_Full_Data %>%
  left_join(outlet_counts, by = "Entity_ID")


# this should not have any issues running on the group set as it is using the same defined variables from above. 

Customer_Full_Data <- Customer_Full_Data |>
  mutate(
    total_ordered = orderedCases_2023 + orderedGallons_2023 + orderedCases_2024 + orderedGallons_2024,
    high_order_volume = if_else(total_ordered >= 400, "High", "Low")
  )


```

```{r}
LMP <- Customer_Full_Data %>% 
  mutate(LMP_Identifier = case_when((LOCAL_MARKET_PARTNER == 1 & CO2_CUSTOMER == 0 & orderedCases_2023==0 & orderedCases_2024 == 0)~"LMP no C02 Fountain Only",(LOCAL_MARKET_PARTNER == 1 & CO2_CUSTOMER == 0 & orderedCases_2023 >= 1 & orderedCases_2024 >= 1)~"LMP No CO2 Cases and Gallons",(LOCAL_MARKET_PARTNER == 1 & CO2_CUSTOMER == 1)~"LMP CO2", TRUE ~ "Not LMP"))

Customer_Full_Data %>% 
  filter(CUSTOMER_NUMBER == 500245736)
```



```{r}
group1 <- LMP %>% 
  filter(LMP_Identifier == "LMP no C02 Fountain Only") %>% 
  select(total_ordered)

group2 <- LMP %>% 
  filter(LMP_Identifier == "LMP No CO2 Cases and Gallons") %>% 
  select(total_ordered)

t.test(group1, group2, var.equal = FALSE)
```
LMP's that do not order CO2 and do not order cases show statistical difference when compared to customers that no not order CO2 as well, but do order cases. This group is minimal though. Only 1324 observations